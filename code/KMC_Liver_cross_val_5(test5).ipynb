{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LiverNet -- Classification task on KMC dataset\n",
    "\n",
    "To request access to the KMC dataset, please contact Prof. Shyam Lal ([shyam.mtec@gmail.com](mailto:shyam.mtec@gmail.com))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fBH9fe-Fp7f2",
    "outputId": "02e24ecc-699a-4d16-cf22-dd4cf8e2add4"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "## Keras \n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.optimizers import *\n",
    "from keras.models import load_model, Model\n",
    "from keras import backend as K\n",
    "from keras.utils import conv_utils\n",
    "\n",
    "## OpenCV\n",
    "import cv2\n",
    "\n",
    "## Utility libraries\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "## Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "## OPTIONAL: MOUNT GOOGLE DRIVE FILESYS WHEN USING STORING DATA ON GOOGLE DRIVE\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTn8wem023f7"
   },
   "source": [
    "## Reading the dataset\n",
    "\n",
    "### Train - Folds 1,2,3,4\n",
    "### Test - Fold 5\n",
    "\n",
    "5-fold cross validation was performed in our training procedure. Here for an example, fold-5 is considered to be the testset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QbafRggdc3In"
   },
   "outputs": [],
   "source": [
    "no_of_classes = 4\n",
    "classes_name = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vZVL5DEV25s8",
    "outputId": "7eab40b7-ace6-4141-ae92-e71c2a4ac1c1"
   },
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_label = []\n",
    "test_data = []\n",
    "test_label = []\n",
    "\n",
    "## Filepath of root directory of the dataset\n",
    "src_path = 'ENTER FILEPATH OF ROOT DIR OF DATASET'\n",
    "\n",
    "## Preparing train data\n",
    "train_folds = ['fold1','fold2','fold3','fold4']\n",
    "test_folds = ['fold5']\n",
    "\n",
    "label=-1\n",
    "\n",
    "## Extract trainset images for this fold config\n",
    "for fold_name in train_folds:\n",
    "  print(\"inside {}\".format(fold_name))\n",
    "  fold_path = src_path + fold_name + '/'\n",
    "  for i,can_type in enumerate(sorted(os.listdir(fold_path))):\n",
    "    label = i\n",
    "    for img_name in sorted(os.listdir(fold_path + can_type + '/')):\n",
    "      img = cv2.imread(fold_path + can_type + '/' + img_name,1)\n",
    "      img = img/255.0\n",
    "      train_data.append(img)\n",
    "      train_label.append(label)\n",
    "      img_hf = cv2.flip(img,0)\n",
    "      train_data.append(img_hf)\n",
    "      train_label.append(label)\n",
    "      img_vf = cv2.flip(img,1)\n",
    "      train_data.append(img_vf)\n",
    "      train_label.append(label)\n",
    "\n",
    "## Extract testset images for this fold config\n",
    "for fold_name in test_folds:\n",
    "  print(\"inside {}\".format(fold_name))\n",
    "  fold_path_test = src_path + fold_name + '/'\n",
    "  for i,can_type in enumerate(sorted(os.listdir(fold_path_test))):\n",
    "    label = i\n",
    "    for img_name in sorted(os.listdir(fold_path_test + can_type + '/')):\n",
    "      img = cv2.imread(fold_path_test + can_type + '/' + img_name,1)\n",
    "      img = img/255.0\n",
    "      test_data.append(img)\n",
    "      test_label.append(label)\n",
    "\n",
    "train_label = tf.keras.utils.to_categorical(train_label)\n",
    "test_label = tf.keras.utils.to_categorical(test_label)\n",
    "\n",
    "train_data = np.array(train_data)\n",
    "test_data = np.array(test_data)\n",
    "train_label = np.array(train_label)\n",
    "test_label = np.array(test_label)\n",
    "\n",
    "print('train data size = {}'.format(train_data.shape))\n",
    "print('train label size = {}'.format(train_label.shape))\n",
    "print('test data size = {}'.format(test_data.shape))\n",
    "print('test label size = {}'.format(test_label.shape))\n",
    "\n",
    "train_data = tf.convert_to_tensor(train_data)\n",
    "test_data = tf.convert_to_tensor(test_data)\n",
    "train_label = tf.convert_to_tensor(train_label)\n",
    "test_label = tf.convert_to_tensor(test_label)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of competitive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhq7aGbOYWYR"
   },
   "source": [
    "## 1. ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hpwHhv1f229J",
    "outputId": "76735323-134d-4910-9f35-b0d19e69b27d"
   },
   "outputs": [],
   "source": [
    "def Resnet_transfer_orig():\n",
    "  base_model_res = tf.keras.applications.ResNet50(weights = None , include_top = False , input_shape=(224,224,3))\n",
    "  x = base_model_res.output\n",
    "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "  pred = tf.keras.layers.Dense(no_of_classes, activation='softmax')(x)\n",
    "  model = tf.keras.models.Model(inputs = base_model_res.input , outputs = pred)\n",
    "  for layer in model.layers[0:]:\n",
    "    layer.trainable = True\n",
    "  \n",
    "  #model = tf.keras.models.Model(inputs = base_model_res.input , outputs = pred)\n",
    "  return model \n",
    "\n",
    "\n",
    "model_Res =  Resnet_transfer_orig()\n",
    "\n",
    "for layer in model_Res.layers:\n",
    "    if hasattr(layer, 'kernel_regularizer'):\n",
    "        layer.kernel_regularizer= regularizers.l2(0.003)\n",
    "    \n",
    "    if hasattr(layer, 'bias_regularizer'):\n",
    "        layer.bias_regularizer= regularizers.l2(0.003)\n",
    "adam = tf.keras.optimizers.Adam(lr = 0.001)\n",
    "model_Res.compile(optimizer = adam , loss = 'categorical_crossentropy' , metrics = [\"acc\"])\n",
    "model_Res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bskCRmrGZP-Q"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eYKAVkkZqNoV",
    "outputId": "0d35f95d-0cb9-4910-8ba4-43fea187c239"
   },
   "outputs": [],
   "source": [
    "\n",
    "def scheduler(epoch, lr):\n",
    "   if epoch < 10:\n",
    "     return lr\n",
    "   else:\n",
    "     return lr * tf.math.exp(-0.1)\n",
    "\n",
    "\n",
    "lrate =  tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "history = model_Res.fit(train_data, train_label, batch_size = 4, verbose=1, \n",
    "                        steps_per_epoch = len(train_data)//4, \n",
    "                         epochs = 40, callbacks = [lrate])\n",
    "                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbbAcX-Adcot"
   },
   "source": [
    "### Results and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "pZXj2ThodbFB",
    "outputId": "dc3df574-c886-40f0-cff8-1554b0672346"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'] , label = 'train_loss')\n",
    "##plt.plot(history.history['val_loss'] , label = 'val_loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"No. of epochs\")\n",
    "plt.ylabel(\"Loss(Categorical Crossentropy)\")\n",
    "plt.title(\"Loss vs. Epoch plot for ResNet50\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['acc'] , label = 'train_acc')\n",
    "##plt.plot(history.history['val_acc'] , label = 'val_acc')\n",
    "plt.legend()\n",
    "plt.xlabel(\"No. of epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs. Epoch plot for ResNet50\")\n",
    "plt.show()\n",
    "\n",
    "print('original')\n",
    "orig_class_indices = np.argmax(test_label,axis=1) \n",
    "print(orig_class_indices)\n",
    "print('\\n')\n",
    "print('predicted')\n",
    "#test_step = test_generator.n//test_generator.batch_size\n",
    "#test_generator.reset()\n",
    "pred = model_Res.predict(test_data ,batch_size=1, verbose = 1)\n",
    "pred_class_indices = np.argmax(pred,axis=1)\n",
    "print(pred_class_indices)\n",
    "print('\\n')\n",
    "\n",
    "#print(train_generator.class_indices)\n",
    "#print('\\n')\n",
    "from sklearn.metrics import precision_score,recall_score,accuracy_score,roc_curve, confusion_matrix, roc_auc_score, auc, f1_score,jaccard_score,classification_report\n",
    "print('Precision {}'.format(precision_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro')))\n",
    "print('Recall {}'.format(recall_score(y_true=orig_class_indices, y_pred=pred_class_indices,average='micro')))\n",
    "print('Accuracy {}'.format(accuracy_score(y_true=orig_class_indices, y_pred=pred_class_indices)))\n",
    "print('F1 {}'.format(f1_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro')))\n",
    "print('jaccard {}'.format(jaccard_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro')))\n",
    "print('confusion_matrix\\n {}'.format(confusion_matrix(y_true=orig_class_indices, y_pred=pred_class_indices)))\n",
    "#print('classification_report\\n {}'.format(classification_report(y_true=orig_class_indices, y_pred=pred_class_indices)))\n",
    "print('\\n\\n')\n",
    "\n",
    "met = np.zeros((no_of_classes+1,5))\n",
    "\n",
    "avg_met = [precision_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro'),\n",
    "           recall_score(y_true=orig_class_indices, y_pred=pred_class_indices,average='micro'),\n",
    "           f1_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro'),\n",
    "           jaccard_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro'),\n",
    "           accuracy_score(y_true=orig_class_indices, y_pred=pred_class_indices)]\n",
    "\n",
    "avg_met = np.array(avg_met)\n",
    "avg_met.round(decimals=2)\n",
    "met[no_of_classes,:] = avg_met\n",
    "\n",
    "classes = classes_name\n",
    "\n",
    "\n",
    "for cl in classes:\n",
    "\n",
    "    print(\"class: \",cl)\n",
    "\n",
    "    a1 = np.uint8(orig_class_indices == cl)\n",
    "    a2 = np.uint8(pred_class_indices == cl)\n",
    "\n",
    "    print('Accuracy {}'.format(accuracy_score(y_true=a1, y_pred=a2)))\n",
    "    print('F1 {}'.format(f1_score(y_true=a1, y_pred=a2)))\n",
    "    print('precision {}'.format(precision_score(y_true=a1, y_pred=a2)))\n",
    "    print('recall {}'.format(recall_score(y_true=a1, y_pred=a2)))\n",
    "\n",
    "    print('jaccard {}'.format(jaccard_score(y_true=a1, y_pred=a2)))\n",
    "    print(\"_______________________________\")\n",
    "\n",
    "    class_met = [precision_score(y_true=a1, y_pred=a2),\n",
    "                 recall_score(y_true=a1, y_pred=a2),\n",
    "                 f1_score(y_true=a1, y_pred=a2),\n",
    "                 jaccard_score(y_true=a1, y_pred=a2),\n",
    "                 accuracy_score(y_true=a1, y_pred=a2)]\n",
    "\n",
    "    class_met = np.array(class_met)\n",
    "    class_met.round(decimals=2)\n",
    "    met[cl,:] =class_met\n",
    "\n",
    "\n",
    "print(met)\n",
    "np.save('/content/drive/MyDrive/LiverNet/KMC_Liver_dataset_folds/Metrics/resnet50_liver_kmc_met_5.npy',met)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "cm = confusion_matrix(y_true=orig_class_indices, y_pred=pred_class_indices)\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "plot_confusion_matrix(cm, classes_name,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix for ResNet50',\n",
    "                        cmap=plt.cm.Blues)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjpISZltiBrh"
   },
   "source": [
    "## 2. DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IvUruJK1iEEq",
    "outputId": "7d49ca05-8293-41c2-df58-0e618a591abd"
   },
   "outputs": [],
   "source": [
    "def DenseNet_transfer_orig():\n",
    "  base_model_dense = tf.keras.applications.DenseNet121(weights = None , include_top = False , input_shape=(224,224,3))\n",
    "  x = base_model_dense.output\n",
    "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "  pred = tf.keras.layers.Dense(no_of_classes, activation='softmax')(x)\n",
    "  model = tf.keras.models.Model(inputs = base_model_dense.input , outputs = pred)\n",
    "  for layer in model.layers[0:]:\n",
    "    layer.trainable = True\n",
    "  \n",
    "  return model \n",
    "\n",
    "\n",
    "model_dense =  DenseNet_transfer_orig()\n",
    "\n",
    "for layer in model_dense.layers:\n",
    "    if hasattr(layer, 'kernel_regularizer'):\n",
    "        layer.kernel_regularizer= regularizers.l2(0.003)\n",
    "    \n",
    "    if hasattr(layer, 'bias_regularizer'):\n",
    "        layer.bias_regularizer= regularizers.l2(0.003)\n",
    "adam = tf.keras.optimizers.Adam(lr = 0.001)\n",
    "model_dense.compile(optimizer = adam , loss = 'categorical_crossentropy' , metrics = [\"acc\"])\n",
    "model_dense.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgdWbfkEiOxe"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zDr72OaCiRWb",
    "outputId": "3fa0d8a4-c529-4dcc-c522-51f67647f160"
   },
   "outputs": [],
   "source": [
    "\n",
    "def scheduler(epoch, lr):\n",
    "   if epoch < 10:\n",
    "     return lr\n",
    "   else:\n",
    "     return lr * tf.math.exp(-0.1)\n",
    "\n",
    "\n",
    "lrate =  tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "history = model_dense.fit(train_data, train_label, batch_size = 4, verbose=1, \n",
    "                         steps_per_epoch = len(train_data)//4, \n",
    "                         epochs = 40, callbacks = [lrate]\n",
    "                          )\n",
    "                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sqmlFzkigsz"
   },
   "source": [
    "### Results and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "I6DHwGjsikQ-",
    "outputId": "1f23323e-ad4a-4666-da48-173553a2e667"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'] , label = 'train_loss')\n",
    "#plt.plot(history.history['val_loss'] , label = 'val_loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"No. of epochs\")\n",
    "plt.ylabel(\"Loss(Categorical Crossentropy)\")\n",
    "plt.title(\"Loss vs. Epoch plot for DenseNet121\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['acc'] , label = 'train_acc')\n",
    "#plt.plot(history.history['val_acc'] , label = 'val_acc')\n",
    "plt.legend()\n",
    "plt.xlabel(\"No. of epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs. Epoch plot for DenseNet121\")\n",
    "plt.show()\n",
    "\n",
    "print('original')\n",
    "orig_class_indices = np.argmax(test_label,axis=1) \n",
    "print(orig_class_indices)\n",
    "print('\\n')\n",
    "print('predicted')\n",
    "#test_step = test_generator.n//test_generator.batch_size\n",
    "#test_generator.reset()\n",
    "pred = model_dense.predict(test_data ,batch_size=1, verbose = 1)\n",
    "pred_class_indices = np.argmax(pred,axis=1)\n",
    "print(pred_class_indices)\n",
    "print('\\n')\n",
    "\n",
    "#print(train_generator.class_indices)\n",
    "#print('\\n')\n",
    "from sklearn.metrics import precision_score,recall_score,accuracy_score,roc_curve, confusion_matrix, roc_auc_score, auc, f1_score,jaccard_score,classification_report\n",
    "print('Precision {}'.format(precision_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro')))\n",
    "print('Recall {}'.format(recall_score(y_true=orig_class_indices, y_pred=pred_class_indices,average='micro')))\n",
    "print('Accuracy {}'.format(accuracy_score(y_true=orig_class_indices, y_pred=pred_class_indices)))\n",
    "print('F1 {}'.format(f1_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro')))\n",
    "print('jaccard {}'.format(jaccard_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro')))\n",
    "print('confusion_matrix\\n {}'.format(confusion_matrix(y_true=orig_class_indices, y_pred=pred_class_indices)))\n",
    "#print('classification_report\\n {}'.format(classification_report(y_true=orig_class_indices, y_pred=pred_class_indices)))\n",
    "print('\\n\\n')\n",
    "\n",
    "met = np.zeros((no_of_classes+1,5))\n",
    "\n",
    "avg_met = [precision_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro'),\n",
    "           recall_score(y_true=orig_class_indices, y_pred=pred_class_indices,average='micro'),\n",
    "           f1_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro'),\n",
    "           jaccard_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro'),\n",
    "           accuracy_score(y_true=orig_class_indices, y_pred=pred_class_indices)]\n",
    "\n",
    "avg_met = np.array(avg_met)\n",
    "avg_met.round(decimals=2)\n",
    "met[no_of_classes,:] = avg_met\n",
    "\n",
    "classes = classes_name\n",
    "\n",
    "\n",
    "for cl in classes:\n",
    "\n",
    "    print(\"class: \",cl)\n",
    "\n",
    "    a1 = np.uint8(orig_class_indices == cl)\n",
    "    a2 = np.uint8(pred_class_indices == cl)\n",
    "\n",
    "    print('Accuracy {}'.format(accuracy_score(y_true=a1, y_pred=a2)))\n",
    "    print('F1 {}'.format(f1_score(y_true=a1, y_pred=a2)))\n",
    "    print('precision {}'.format(precision_score(y_true=a1, y_pred=a2)))\n",
    "    print('recall {}'.format(recall_score(y_true=a1, y_pred=a2)))\n",
    "\n",
    "    print('jaccard {}'.format(jaccard_score(y_true=a1, y_pred=a2)))\n",
    "    print(\"_______________________________\")\n",
    "\n",
    "    class_met = [precision_score(y_true=a1, y_pred=a2),\n",
    "                 recall_score(y_true=a1, y_pred=a2),\n",
    "                 f1_score(y_true=a1, y_pred=a2),\n",
    "                 jaccard_score(y_true=a1, y_pred=a2),\n",
    "                 accuracy_score(y_true=a1, y_pred=a2)]\n",
    "\n",
    "    class_met = np.array(class_met)\n",
    "    class_met.round(decimals=2)\n",
    "    met[cl,:] =class_met\n",
    "\n",
    "\n",
    "print(met)\n",
    "np.save('/content/drive/MyDrive/LiverNet/KMC_Liver_dataset_folds/Metrics/densenet121_liver_kmc_met_5.npy',met)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "cm = confusion_matrix(y_true=orig_class_indices, y_pred=pred_class_indices)\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "plot_confusion_matrix(cm, classes_name,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix for DenseNet121',\n",
    "                        cmap=plt.cm.Blues)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wsZqof_0jrLC"
   },
   "source": [
    "## 3. InceptionResNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GZykfDT4juad",
    "outputId": "12904bd6-f92c-475f-95f2-c5e152109832"
   },
   "outputs": [],
   "source": [
    "def InceptionResNet_transfer_orig():\n",
    "  base_model_inres = tf.keras.applications.InceptionResNetV2(weights = None , include_top = False , input_shape=(224,224,3))\n",
    "  x = base_model_inres.output\n",
    "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "  pred = tf.keras.layers.Dense(no_of_classes, activation='softmax')(x)\n",
    "  model = tf.keras.models.Model(inputs = base_model_inres.input , outputs = pred)\n",
    "  for layer in model.layers[0:]:\n",
    "    layer.trainable = True\n",
    "  \n",
    "  #model = tf.keras.models.Model(inputs = base_model_res.input , outputs = pred)\n",
    "  return model \n",
    "\n",
    "\n",
    "model_inres =  InceptionResNet_transfer_orig()\n",
    "\n",
    "for layer in model_inres.layers:\n",
    "    if hasattr(layer, 'kernel_regularizer'):\n",
    "        layer.kernel_regularizer= regularizers.l2(0.003)\n",
    "    \n",
    "    if hasattr(layer, 'bias_regularizer'):\n",
    "        layer.bias_regularizer= regularizers.l2(0.003)\n",
    "adam = tf.keras.optimizers.Adam(lr = 0.001)\n",
    "model_inres.compile(optimizer = adam , loss = 'categorical_crossentropy' , metrics = [\"acc\"])\n",
    "model_inres.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1k5xDOD4mDWu"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPG7PWwemFQz",
    "outputId": "5c7b25b3-0715-4ae4-8203-a35a860f484b"
   },
   "outputs": [],
   "source": [
    "\n",
    "def scheduler(epoch, lr):\n",
    "   if epoch < 10:\n",
    "     return lr\n",
    "   else:\n",
    "     return lr * tf.math.exp(-0.1)\n",
    "\n",
    "\n",
    "lrate =  tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "\n",
    "history = model_inres.fit(train_data, train_label, batch_size = 4, verbose=1, \n",
    "                        steps_per_epoch = len(train_data)//4, \n",
    "                         epochs = 40,callbacks = [lrate]\n",
    "                        )\n",
    "                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1EvLCZmmQd6"
   },
   "source": [
    "### Results and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DhT4tMhKmTUC",
    "outputId": "710ccbe0-4b05-427b-f91c-015973b45ede"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'] , label = 'train_loss')\n",
    "#plt.plot(history.history['val_loss'] , label = 'val_loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"No. of epochs\")\n",
    "plt.ylabel(\"Loss(Categorical Crossentropy)\")\n",
    "plt.title(\"Loss vs. Epoch plot for InceptionResNetV2\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['acc'] , label = 'train_acc')\n",
    "#plt.plot(history.history['val_acc'] , label = 'val_acc')\n",
    "plt.legend()\n",
    "plt.xlabel(\"No. of epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs. Epoch plot for InceptionResNetV2\")\n",
    "plt.show()\n",
    "\n",
    "print('original')\n",
    "orig_class_indices = np.argmax(test_label,axis=1) \n",
    "print(orig_class_indices)\n",
    "print('\\n')\n",
    "print('predicted')\n",
    "#test_step = test_generator.n//test_generator.batch_size\n",
    "#test_generator.reset()\n",
    "pred = model_inres.predict(test_data ,batch_size=1 , verbose = 1)\n",
    "pred_class_indices = np.argmax(pred,axis=1)\n",
    "print(pred_class_indices)\n",
    "print('\\n')\n",
    "\n",
    "#print(train_generator.class_indices)\n",
    "#print('\\n')\n",
    "from sklearn.metrics import precision_score,recall_score,accuracy_score,roc_curve, confusion_matrix, roc_auc_score, auc, f1_score,jaccard_score,classification_report\n",
    "print('Precision {}'.format(precision_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro')))\n",
    "print('Recall {}'.format(recall_score(y_true=orig_class_indices, y_pred=pred_class_indices,average='micro')))\n",
    "print('Accuracy {}'.format(accuracy_score(y_true=orig_class_indices, y_pred=pred_class_indices)))\n",
    "print('F1 {}'.format(f1_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro')))\n",
    "print('jaccard {}'.format(jaccard_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro')))\n",
    "print('confusion_matrix\\n {}'.format(confusion_matrix(y_true=orig_class_indices, y_pred=pred_class_indices)))\n",
    "#print('classification_report\\n {}'.format(classification_report(y_true=orig_class_indices, y_pred=pred_class_indices)))\n",
    "print('\\n\\n')\n",
    "\n",
    "met = np.zeros((no_of_classes+1,5))\n",
    "\n",
    "avg_met = [precision_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro'),\n",
    "           recall_score(y_true=orig_class_indices, y_pred=pred_class_indices,average='micro'),\n",
    "           f1_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro'),\n",
    "           jaccard_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro'),\n",
    "           accuracy_score(y_true=orig_class_indices, y_pred=pred_class_indices)]\n",
    "\n",
    "avg_met = np.array(avg_met)\n",
    "avg_met.round(decimals=2)\n",
    "met[no_of_classes,:] = avg_met\n",
    "\n",
    "classes = classes_name\n",
    "\n",
    "\n",
    "for cl in classes:\n",
    "\n",
    "    print(\"class: \",cl)\n",
    "\n",
    "    a1 = np.uint8(orig_class_indices == cl)\n",
    "    a2 = np.uint8(pred_class_indices == cl)\n",
    "\n",
    "    print('Accuracy {}'.format(accuracy_score(y_true=a1, y_pred=a2)))\n",
    "    print('F1 {}'.format(f1_score(y_true=a1, y_pred=a2)))\n",
    "    print('precision {}'.format(precision_score(y_true=a1, y_pred=a2)))\n",
    "    print('recall {}'.format(recall_score(y_true=a1, y_pred=a2)))\n",
    "\n",
    "    print('jaccard {}'.format(jaccard_score(y_true=a1, y_pred=a2)))\n",
    "    print(\"_______________________________\")\n",
    "\n",
    "    class_met = [precision_score(y_true=a1, y_pred=a2),\n",
    "                 recall_score(y_true=a1, y_pred=a2),\n",
    "                 f1_score(y_true=a1, y_pred=a2),\n",
    "                 jaccard_score(y_true=a1, y_pred=a2),\n",
    "                 accuracy_score(y_true=a1, y_pred=a2)]\n",
    "\n",
    "    class_met = np.array(class_met)\n",
    "    class_met.round(decimals=2)\n",
    "    met[cl,:] =class_met\n",
    "\n",
    "\n",
    "print(met)\n",
    "np.save('/content/drive/MyDrive/LiverNet/KMC_Liver_dataset_folds/Metrics/InceptionResNetV2_liver_kmc_met_5.npy',met)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "cm = confusion_matrix(y_true=orig_class_indices, y_pred=pred_class_indices)\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "plot_confusion_matrix(cm, classes_name,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix for InceptionResNetV2',\n",
    "                        cmap=plt.cm.Blues)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_aH7UBK2nBYj"
   },
   "source": [
    "## 4. BreastNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mX9Tj0D1nHEm",
    "outputId": "25dc4181-ba5c-471d-8183-dc7fcad6f9ee"
   },
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.optimizers import *\n",
    "from keras.models import load_model, Model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def cbam_block(cbam_feature, ratio=8):\n",
    "    \"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n",
    "    As described in https://arxiv.org/abs/1807.06521.\n",
    "    \"\"\"\n",
    "    \n",
    "    cbam_feature = channel_attention(cbam_feature, ratio)\n",
    "    cbam_feature = spatial_attention(cbam_feature)\n",
    "    return cbam_feature\n",
    "\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    channel = input_feature.shape[channel_axis]\n",
    "    \n",
    "    shared_layer_one = Dense(channel//ratio,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    shared_layer_two = Dense(channel,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling2D()(input_feature)    \n",
    "    avg_pool = Reshape((1,1,channel))(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1,1,channel)\n",
    "    avg_pool = shared_layer_one(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1,1,channel//ratio)\n",
    "    avg_pool = shared_layer_two(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1,1,channel)\n",
    "    \n",
    "    max_pool = GlobalMaxPooling2D()(input_feature)\n",
    "    max_pool = Reshape((1,1,channel))(max_pool)\n",
    "    assert max_pool.shape[1:] == (1,1,channel)\n",
    "    max_pool = shared_layer_one(max_pool)\n",
    "    assert max_pool.shape[1:] == (1,1,channel//ratio)\n",
    "    max_pool = shared_layer_two(max_pool)\n",
    "    assert max_pool.shape[1:] == (1,1,channel)\n",
    "    \n",
    "    cbam_feature = Add()([avg_pool,max_pool])\n",
    "    cbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "    \n",
    "    return multiply([input_feature, cbam_feature])\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "    kernel_size = 7\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        channel = input_feature.shape[1]\n",
    "        cbam_feature = Permute((2,3,1))(input_feature)\n",
    "    else:\n",
    "        channel = input_feature.shape[-1]\n",
    "        cbam_feature = input_feature\n",
    "    \n",
    "    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert avg_pool.shape[-1] == 1\n",
    "    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert max_pool.shape[-1] == 1\n",
    "    concat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "    assert concat.shape[-1] == 2\n",
    "    cbam_feature = Conv2D(filters = 1,\n",
    "                    kernel_size=kernel_size,\n",
    "                    strides=1,\n",
    "                    padding='same',\n",
    "                    activation='sigmoid',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    use_bias=False)(concat)\t\n",
    "    assert cbam_feature.shape[-1] == 1\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "        \n",
    "    return multiply([input_feature, cbam_feature])\n",
    "\n",
    "\n",
    "\n",
    "def residual_block(y, nb_channels, _strides=(1, 1), _project_shortcut=False):\n",
    "    shortcut = y\n",
    "\n",
    "    # down-sampling is performed with a stride of 2\n",
    "    y = Conv2D(nb_channels, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "    y = LeakyReLU()(y)\n",
    "\n",
    "    y = Conv2D(nb_channels, kernel_size=(3, 3), strides=(1, 1), padding='same')(y)\n",
    "    y = BatchNormalization()(y)\n",
    "\n",
    "    # identity shortcuts used directly when the input and output are of the same dimensions\n",
    "    if _project_shortcut or _strides != (1, 1):\n",
    "        # when the dimensions increase projection shortcut is used to match dimensions (done by 1×1 convolutions)\n",
    "        # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
    "        shortcut = Conv2D(nb_channels, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    y = add([shortcut, y])\n",
    "    y = LeakyReLU()(y)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    dropRate = 0.3\n",
    "    \n",
    "    init = Input((224,224,3))\n",
    "    x = Conv2D(32, (3, 3), activation=None, padding='same')(init) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(32, (3, 3), activation=None, padding='same')(x) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x1 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation=None, padding='same')(x1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = cbam_block(x)\n",
    "    x = residual_block(x, 64)\n",
    "    x2 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation=None, padding='same')(x2)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = cbam_block(x)\n",
    "    x = residual_block(x, 128)\n",
    "    x3 = MaxPooling2D((2,2))(x)\n",
    "    \n",
    "    ginp1 = UpSampling2D(size=(2, 2), interpolation='bilinear')(x1)\n",
    "    ginp2 = UpSampling2D(size=(4, 4), interpolation='bilinear')(x2)\n",
    "    ginp3 = UpSampling2D(size=(8, 8), interpolation='bilinear')(x3)\n",
    "    \n",
    "    hypercolumn = Concatenate()([ginp1, ginp2, ginp3]) \n",
    "    gap = GlobalAveragePooling2D()(hypercolumn)\n",
    "\n",
    "    x = Dense(256, activation=None)(gap)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropRate)(x)\n",
    "    \n",
    "    x = Dense(256, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    y = Dense(no_of_classes, activation='softmax')(x)\n",
    "   \n",
    "    model = Model(init, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "model_breastnet = create_model()\n",
    "model_breastnet.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [\"acc\"])\n",
    "model_breastnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yFMSxiHnM3G"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E6Bo8ZhunOUV",
    "outputId": "66c20036-9251-4580-faaa-8c002178dc3b"
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "   if epoch < 10:\n",
    "     return lr\n",
    "   else:\n",
    "     return lr * tf.math.exp(-0.1)\n",
    "\n",
    "\n",
    "lrate =  tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "\n",
    "history = model_breastnet.fit(train_data, train_label, batch_size = 4, verbose=1, \n",
    "                        steps_per_epoch = len(train_data)//4, \n",
    "                        epochs = 40,callbacks = [lrate]\n",
    "                       )\n",
    "                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-k8xFx4nXk4"
   },
   "source": [
    "### Results and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KUuPOliar_fl",
    "outputId": "ed5deb7f-cc67-42b0-fb3f-f5934b4360ef"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'] , label = 'train_loss')\n",
    "#plt.plot(history.history['val_loss'] , label = 'val_loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"No. of epochs\")\n",
    "plt.ylabel(\"Loss(Categorical Crossentropy)\")\n",
    "plt.title(\"Loss vs. Epoch plot for BreastNet\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['acc'] , label = 'train_acc')\n",
    "#plt.plot(history.history['val_acc'] , label = 'val_acc')\n",
    "plt.legend()\n",
    "plt.xlabel(\"No. of epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs. Epoch plot for BreastNet\")\n",
    "plt.show()\n",
    "\n",
    "print('original')\n",
    "orig_class_indices = np.argmax(test_label,axis=1) \n",
    "print(orig_class_indices)\n",
    "print('\\n')\n",
    "print('predicted')\n",
    "#test_step = test_generator.n//test_generator.batch_size\n",
    "#test_generator.reset()\n",
    "pred = model_breastnet.predict(test_data ,batch_size=1, verbose = 1)\n",
    "pred_class_indices = np.argmax(pred,axis=1)\n",
    "print(pred_class_indices)\n",
    "print('\\n')\n",
    "\n",
    "#print(train_generator.class_indices)\n",
    "#print('\\n')\n",
    "from sklearn.metrics import precision_score,recall_score,accuracy_score,roc_curve, confusion_matrix, roc_auc_score, auc, f1_score,jaccard_score,classification_report\n",
    "print('Precision {}'.format(precision_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro')))\n",
    "print('Recall {}'.format(recall_score(y_true=orig_class_indices, y_pred=pred_class_indices,average='micro')))\n",
    "print('Accuracy {}'.format(accuracy_score(y_true=orig_class_indices, y_pred=pred_class_indices)))\n",
    "print('F1 {}'.format(f1_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro')))\n",
    "print('jaccard {}'.format(jaccard_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro')))\n",
    "print('confusion_matrix\\n {}'.format(confusion_matrix(y_true=orig_class_indices, y_pred=pred_class_indices)))\n",
    "#print('classification_report\\n {}'.format(classification_report(y_true=orig_class_indices, y_pred=pred_class_indices)))\n",
    "print('\\n\\n')\n",
    "\n",
    "met = np.zeros((no_of_classes+1,5))\n",
    "\n",
    "avg_met = [precision_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro'),\n",
    "           recall_score(y_true=orig_class_indices, y_pred=pred_class_indices,average='micro'),\n",
    "           f1_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro'),\n",
    "           jaccard_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro'),\n",
    "           accuracy_score(y_true=orig_class_indices, y_pred=pred_class_indices)]\n",
    "\n",
    "avg_met = np.array(avg_met)\n",
    "avg_met.round(decimals=2)\n",
    "met[no_of_classes,:] = avg_met\n",
    "\n",
    "classes = classes_name\n",
    "\n",
    "\n",
    "for cl in classes:\n",
    "\n",
    "    print(\"class: \",cl)\n",
    "\n",
    "    a1 = np.uint8(orig_class_indices == cl)\n",
    "    a2 = np.uint8(pred_class_indices == cl)\n",
    "\n",
    "    print('Accuracy {}'.format(accuracy_score(y_true=a1, y_pred=a2)))\n",
    "    print('F1 {}'.format(f1_score(y_true=a1, y_pred=a2)))\n",
    "    print('precision {}'.format(precision_score(y_true=a1, y_pred=a2)))\n",
    "    print('recall {}'.format(recall_score(y_true=a1, y_pred=a2)))\n",
    "\n",
    "    print('jaccard {}'.format(jaccard_score(y_true=a1, y_pred=a2)))\n",
    "    print(\"_______________________________\")\n",
    "\n",
    "    class_met = [precision_score(y_true=a1, y_pred=a2),\n",
    "                 recall_score(y_true=a1, y_pred=a2),\n",
    "                 f1_score(y_true=a1, y_pred=a2),\n",
    "                 jaccard_score(y_true=a1, y_pred=a2),\n",
    "                 accuracy_score(y_true=a1, y_pred=a2)]\n",
    "\n",
    "    class_met = np.array(class_met)\n",
    "    class_met.round(decimals=2)\n",
    "    met[cl,:] =class_met\n",
    "\n",
    "\n",
    "print(met)\n",
    "np.save('/content/drive/MyDrive/LiverNet/KMC_Liver_dataset_folds/Metrics/BreastNet_liver_kmc_met_5.npy',met)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "cm = confusion_matrix(y_true=orig_class_indices, y_pred=pred_class_indices)\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "plot_confusion_matrix(cm, classes_name,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix for BreastNet',\n",
    "                        cmap=plt.cm.Blues)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jze3874ipFAc"
   },
   "source": [
    "## 5. IRRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zQAil-CvsHNa",
    "outputId": "b4576dc3-ceb3-47be-b632-5b29dc856724"
   },
   "outputs": [],
   "source": [
    "def RCNN(filedepth, input):\n",
    "  conv1 = tf.keras.layers.Conv2D(filters=filedepth, kernel_size=(3, 3), strides=(1, 1), padding='same',activation='relu')(input)\n",
    "  stack2 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "\n",
    "  RCL = tf.keras.layers.Conv2D(filters=filedepth, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu')\n",
    "\n",
    "  conv2 = RCL(stack2)\n",
    "  stack3 = tf.keras.layers.Add()([conv1, conv2])\n",
    "  stack4 = tf.keras.layers.BatchNormalization()(stack3)\n",
    "\n",
    "  conv3 = tf.keras.layers.Conv2D(filters=filedepth, kernel_size=(3, 3), strides=(1, 1), padding='same',activation='relu', weights=RCL.get_weights())(stack4)\n",
    "  stack5 =  tf.keras.layers.Add()([conv1, conv3])\n",
    "  stack6 = tf.keras.layers.BatchNormalization()(stack5)\n",
    "\n",
    "  conv4 = tf.keras.layers.Conv2D(filters=filedepth, kernel_size=(3, 3), strides=(1, 1), padding='same',activation='relu', weights=RCL.get_weights())(stack6)\n",
    "  stack7 =  tf.keras.layers.Add()([conv1, conv4])\n",
    "  stack8 = tf.keras.layers.BatchNormalization()(stack7)\n",
    "  return stack8\n",
    "\n",
    "\n",
    "def inception_block(input,fil):\n",
    "  conv1 = tf.keras.layers.Conv2D(filters = fil/4, kernel_size = (1,1), strides = (1,1), padding='same')(input)\n",
    "  conv1 = tf.keras.layers.BatchNormalization()(conv1)\n",
    "  conv1 = tf.keras.layers.Activation('relu')(conv1)\n",
    "\n",
    "  conv2 = tf.keras.layers.Conv2D(filters = fil/4, kernel_size = (1,1), strides = (1,1), padding='same')(input)\n",
    "  conv2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "  conv2 = tf.keras.layers.Activation('relu')(conv2)\n",
    "  conv2 = tf.keras.layers.Conv2D(filters = fil/4, kernel_size = (3,3), strides = (1,1), padding='same')(conv2)\n",
    "  conv2 = tf.keras.layers.BatchNormalization()(conv2)\n",
    "  conv2 = tf.keras.layers.Activation('relu')(conv2)\n",
    "\n",
    "  conv3 = tf.keras.layers.Conv2D(filters = fil/4, kernel_size = (1,1), strides = (1,1), padding='same')(input)\n",
    "  conv3 = tf.keras.layers.BatchNormalization()(conv3)\n",
    "  conv3 = tf.keras.layers.Activation('relu')(conv3)\n",
    "  conv3 = tf.keras.layers.Conv2D(filters = fil/4, kernel_size = (5,5), strides = (1,1), padding='same')(conv3)\n",
    "  conv3 = tf.keras.layers.BatchNormalization()(conv3)\n",
    "  conv3 = tf.keras.layers.Activation('relu')(conv3)\n",
    "\n",
    "  conv4 = tf.keras.layers.MaxPooling2D(pool_size = (3,3),strides = (1,1),padding = 'same')(input)\n",
    "  conv4 = tf.keras.layers.Conv2D(filters = fil/4, kernel_size = [1,1], strides = (1,1), padding='same')(conv4)\n",
    "  conv4 = tf.keras.layers.BatchNormalization()(conv4)\n",
    "  conv4 = tf.keras.layers.Activation('relu')(conv4)\n",
    "\n",
    "  output = tf.keras.layers.Concatenate()([conv1,conv2,conv3,conv4])\n",
    "  return output\n",
    "\n",
    "\n",
    "def irrcnn_block(input,fil):\n",
    "  rec = RCNN(fil/2,input)\n",
    "  rec = RCNN(fil/2,rec)\n",
    "\n",
    "  inc = inception_block(input,fil/2)\n",
    "  inc = inception_block(input,fil/2)\n",
    "\n",
    "  ircnn = tf.keras.layers.Concatenate()([rec,inc])\n",
    "  \n",
    "  conv =   tf.keras.layers.Conv2D(fil, kernel_size=(1, 1), \n",
    "                         activation='relu', padding='same', kernel_initializer='he_normal')(input)\n",
    "  \n",
    "  out = tf.keras.layers.Add()([ircnn,conv])\n",
    "  out = tf.keras.layers.LeakyReLU()(out)\n",
    "  return out\n",
    "\n",
    "\n",
    "\n",
    "def get_model():\n",
    "  input = tf.keras.layers.Input(shape=(224, 224, 3))\n",
    "  conv1 = tf.keras.layers.Conv2D(16, kernel_size=(7, 7), activation='relu', padding='same', kernel_initializer='he_normal')(input)\n",
    "\n",
    "  block1 = irrcnn_block(conv1, 32)\n",
    "  block2 = irrcnn_block(block1, 32)\n",
    "  block3 = irrcnn_block(block2,32)\n",
    "\n",
    "  pool1 = tf.keras.layers.MaxPooling2D((3,3), (2,2))(block3)\n",
    "\t\n",
    "  block4 = irrcnn_block(pool1, 64)\n",
    "  block5 = irrcnn_block(block4, 64)\n",
    "  block6 = irrcnn_block(block5,64)\n",
    "\n",
    "  pool2 = tf.keras.layers.MaxPooling2D((3, 3), (2,2))(block6)\n",
    "\n",
    "  block7 = irrcnn_block(pool2, 128)\n",
    "  block8 = irrcnn_block(block7, 128)\n",
    "  block9 = irrcnn_block(block8,128)\t\n",
    "\n",
    "  pool3 = tf.keras.layers.MaxPooling2D((3, 3), (2,2))(block9)\n",
    "\n",
    "  block10 = irrcnn_block(pool3, 256)\n",
    "  block11 = irrcnn_block(block10, 256)\n",
    "  block12 = irrcnn_block(block11,256)\t\n",
    "\n",
    "  global_pool = tf.keras.layers.GlobalAveragePooling2D()(block12)\n",
    "\n",
    "  output = tf.keras.layers.Dense(no_of_classes, activation='softmax')(global_pool)\n",
    "\t\n",
    "  model = tf.keras.models.Model(inputs=input, outputs=output)\n",
    "  return model\n",
    "\n",
    "\n",
    "model_irrcnn = get_model()\n",
    "for layer in model_irrcnn.layers:\n",
    "    if hasattr(layer, 'kernel_regularizer'):\n",
    "        layer.kernel_regularizer= regularizers.l2(0.003)\n",
    "    \n",
    "    if hasattr(layer, 'bias_regularizer'):\n",
    "        layer.bias_regularizer= regularizers.l2(0.003)\n",
    "model_irrcnn.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [\"acc\"])\n",
    "model_irrcnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXpHugFRqTcD"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x4nuYOt0sjOE",
    "outputId": "af000717-b0ea-4cb7-ca59-0836ade9761c"
   },
   "outputs": [],
   "source": [
    "\n",
    "def scheduler(epoch, lr):\n",
    "   if epoch < 10:\n",
    "     return lr\n",
    "   else:\n",
    "     return lr * tf.math.exp(-0.1)\n",
    "\n",
    "\n",
    "lrate =  tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "history = model_irrcnn.fit(train_data, train_label, batch_size = 4, verbose=1, \n",
    "                         steps_per_epoch = len(train_data)//4, \n",
    "                         epochs = 40,callbacks = [lrate]\n",
    "                        )\n",
    "                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Dqp7xJhqhI6"
   },
   "source": [
    "### Results and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wiM3pUQmsgIP",
    "outputId": "410915f6-de71-415f-a3ef-c0a9b2030a91"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'] , label = 'train_loss')\n",
    "#plt.plot(history.history['val_loss'] , label = 'val_loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"No. of epochs\")\n",
    "plt.ylabel(\"Loss(Categorical Crossentropy)\")\n",
    "plt.title(\"Loss vs. Epoch plot for IRRCNN\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['acc'] , label = 'train_acc')\n",
    "#plt.plot(history.history['val_acc'] , label = 'val_acc')\n",
    "plt.legend()\n",
    "plt.xlabel(\"No. of epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs. Epoch plot for IRRCNN\")\n",
    "plt.show()\n",
    "\n",
    "print('original')\n",
    "orig_class_indices = np.argmax(test_label,axis=1) \n",
    "print(orig_class_indices)\n",
    "print('\\n')\n",
    "print('predicted')\n",
    "#test_step = test_generator.n//test_generator.batch_size\n",
    "#test_generator.reset()\n",
    "pred = model_irrcnn.predict(test_data ,batch_size=1, verbose = 1)\n",
    "pred_class_indices = np.argmax(pred,axis=1)\n",
    "print(pred_class_indices)\n",
    "print('\\n')\n",
    "\n",
    "#print(train_generator.class_indices)\n",
    "#print('\\n')\n",
    "from sklearn.metrics import precision_score,recall_score,accuracy_score,roc_curve, confusion_matrix, roc_auc_score, auc, f1_score,jaccard_score,classification_report\n",
    "print('Precision {}'.format(precision_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro')))\n",
    "print('Recall {}'.format(recall_score(y_true=orig_class_indices, y_pred=pred_class_indices,average='micro')))\n",
    "print('Accuracy {}'.format(accuracy_score(y_true=orig_class_indices, y_pred=pred_class_indices)))\n",
    "print('F1 {}'.format(f1_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro')))\n",
    "print('jaccard {}'.format(jaccard_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro')))\n",
    "print('confusion_matrix\\n {}'.format(confusion_matrix(y_true=orig_class_indices, y_pred=pred_class_indices)))\n",
    "#print('classification_report\\n {}'.format(classification_report(y_true=orig_class_indices, y_pred=pred_class_indices)))\n",
    "print('\\n\\n')\n",
    "\n",
    "met = np.zeros((no_of_classes+1,5))\n",
    "\n",
    "avg_met = [precision_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro'),\n",
    "           recall_score(y_true=orig_class_indices, y_pred=pred_class_indices,average='micro'),\n",
    "           f1_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro'),\n",
    "           jaccard_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro'),\n",
    "           accuracy_score(y_true=orig_class_indices, y_pred=pred_class_indices)]\n",
    "\n",
    "avg_met = np.array(avg_met)\n",
    "avg_met.round(decimals=2)\n",
    "met[no_of_classes,:] = avg_met\n",
    "\n",
    "classes = classes_name\n",
    "\n",
    "\n",
    "for cl in classes:\n",
    "\n",
    "    print(\"class: \",cl)\n",
    "\n",
    "    a1 = np.uint8(orig_class_indices == cl)\n",
    "    a2 = np.uint8(pred_class_indices == cl)\n",
    "\n",
    "    print('Accuracy {}'.format(accuracy_score(y_true=a1, y_pred=a2)))\n",
    "    print('F1 {}'.format(f1_score(y_true=a1, y_pred=a2)))\n",
    "    print('precision {}'.format(precision_score(y_true=a1, y_pred=a2)))\n",
    "    print('recall {}'.format(recall_score(y_true=a1, y_pred=a2)))\n",
    "\n",
    "    print('jaccard {}'.format(jaccard_score(y_true=a1, y_pred=a2)))\n",
    "    print(\"_______________________________\")\n",
    "\n",
    "    class_met = [precision_score(y_true=a1, y_pred=a2),\n",
    "                 recall_score(y_true=a1, y_pred=a2),\n",
    "                 f1_score(y_true=a1, y_pred=a2),\n",
    "                 jaccard_score(y_true=a1, y_pred=a2),\n",
    "                 accuracy_score(y_true=a1, y_pred=a2)]\n",
    "\n",
    "    class_met = np.array(class_met)\n",
    "    class_met.round(decimals=2)\n",
    "    met[cl,:] =class_met\n",
    "\n",
    "\n",
    "print(met)\n",
    "np.save('/content/drive/MyDrive/LiverNet/KMC_Liver_dataset_folds/Metrics/IRRCNN_liver_kmc_met_5.npy',met)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "cm = confusion_matrix(y_true=orig_class_indices, y_pred=pred_class_indices)\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "plot_confusion_matrix(cm, classes_name,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix for BreastNet',\n",
    "                        cmap=plt.cm.Blues)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6If-qszBWhr"
   },
   "source": [
    "# LiverNet -- Proposed model\n",
    "\n",
    "[Link to paper](https://link.springer.com/article/10.1007/s11548-021-02410-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iurC8RTfBdK1",
    "outputId": "5178ffe0-1088-4b3c-ed44-093374507af7"
   },
   "outputs": [],
   "source": [
    "def aspp(x,num_fil,input_shape,out_stride):\n",
    "\n",
    "    \"\"\"\n",
    "        ASPP Block\n",
    "        \n",
    "        Arguments:\n",
    "        \n",
    "            x: input feature map to the ASPP block\n",
    "            input_shape: input shape of the feature map\n",
    "            out_stride: the output stride\n",
    "            \n",
    "        Returns: \n",
    "            \n",
    "            output feature map after processing\n",
    "    \"\"\"\n",
    "    \n",
    "    b0=Conv2D(num_fil,(1,1),padding=\"same\",use_bias=False)(x)\n",
    "    b0=BatchNormalization()(b0)\n",
    "    b0=Activation(\"relu\")(b0)\n",
    "\n",
    "    b1=DepthwiseConv2D((3,3),dilation_rate=(2,2),padding=\"same\",use_bias=False)(x)\n",
    "    b1=BatchNormalization()(b1)\n",
    "    b1=Activation(\"relu\")(b1)\n",
    "    b1=Conv2D(num_fil,(1,1),padding=\"same\",dilation_rate = (1,1), use_bias=False)(x)\n",
    "    b1=BatchNormalization()(b1)\n",
    "    b1=Activation(\"relu\")(b1)\n",
    "\n",
    "    b2=DepthwiseConv2D((3,3),dilation_rate=(3,3),padding=\"same\",use_bias=False)(x)\n",
    "    b2=BatchNormalization()(b2)\n",
    "    b2=Activation(\"relu\")(b2)\n",
    "    b2=Conv2D(num_fil, (1,1) , padding=\"same\", use_bias=False)(x)\n",
    "    b2=BatchNormalization()(b2)\n",
    "    b2=Activation(\"relu\")(b2)\t\n",
    "\n",
    "    b3=DepthwiseConv2D((3,3),dilation_rate=(6,6),padding=\"same\",use_bias=False)(x)\n",
    "    b3=BatchNormalization()(b3)\n",
    "    b3=Activation(\"relu\")(b3)\n",
    "    b3=Conv2D(num_fil, (1,1) , padding=\"same\", use_bias=False)(x)\n",
    "    b3=BatchNormalization()(b2)\n",
    "    b3=Activation(\"relu\")(b2)\t\n",
    "\n",
    "    b5=DepthwiseConv2D((3,3),dilation_rate=(8,8),padding=\"same\",use_bias=False)(x)\n",
    "    b5=BatchNormalization()(b5)\n",
    "    b5=Activation(\"relu\")(b5)\n",
    "    b5=Conv2D(num_fil,(1,1),padding=\"same\",use_bias=False)(b5)\n",
    "    b5=BatchNormalization()(b5)\n",
    "    b5=Activation(\"relu\")(b5)\n",
    "    \n",
    "\n",
    "\n",
    "    out_shape=int(input_shape[0]/out_stride)\n",
    "    b4=AveragePooling2D(pool_size=(out_shape,out_shape))(x)\n",
    "    b4=Conv2D(num_fil,(1,1),padding=\"same\",use_bias=False)(b4)\n",
    "    b4=BatchNormalization()(b4)\n",
    "    b4=Activation(\"relu\")(b4)\n",
    "    b4=BilinearUpsampling((out_shape,out_shape))(b4)\n",
    "\n",
    "    x=Concatenate()([b4,b0,b1,b2,b3,b5])\n",
    "    \n",
    "    x = tf.keras.layers.Conv2D(num_fil , kernel_size = (1,1) , padding = 'same')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def normalize_data_format(value):\n",
    "    if value is None:\n",
    "        value = K.image_data_format()\n",
    "    data_format = value.lower()\n",
    "    if data_format not in {'channels_first', 'channels_last'}:\n",
    "        raise ValueError('The `data_format` argument must be one of '\n",
    "                         '\"channels_first\", \"channels_last\". Received: ' +\n",
    "                         str(value))\n",
    "    return data_format\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cbam_block(cbam_feature, ratio=8):\n",
    "   \n",
    "    \"\"\"\n",
    "        CBAM block\n",
    "        \n",
    "        Arguments:\n",
    "        \n",
    "            cbam_feature: input feature map to CBAM block\n",
    "            ratio: channel division ratio in channel attention module\n",
    "            \n",
    "        Returns:\n",
    "        \n",
    "            output feature map after processing in CBAM block\n",
    "    \"\"\"\n",
    "\n",
    "    cbam_feature = channel_attention(cbam_feature, ratio)\n",
    "    cbam_feature = spatial_attention(cbam_feature)\n",
    "    return cbam_feature\n",
    "\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "    \n",
    "    \"\"\"\n",
    "        Channel attention module\n",
    "        \n",
    "        Arguments:\n",
    "            \n",
    "            input_feature: input feature map to channel attention module\n",
    "            ratio: channel reduction ratio\n",
    "            \n",
    "        Returns:\n",
    "        \n",
    "            the product of the channel attention map and input feature map \n",
    "    \"\"\"\n",
    "    \n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    channel = input_feature.shape[channel_axis]\n",
    "    \n",
    "    shared_layer_one = Dense(channel//ratio,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    shared_layer_two = Dense(channel,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    \n",
    "    avg_pool = GlobalAveragePooling2D()(input_feature)    \n",
    "    avg_pool = Reshape((1,1,channel))(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1,1,channel)\n",
    "    avg_pool = shared_layer_one(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1,1,channel//ratio)\n",
    "    avg_pool = shared_layer_two(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1,1,channel)\n",
    "    \n",
    "    max_pool = GlobalMaxPooling2D()(input_feature)\n",
    "    max_pool = Reshape((1,1,channel))(max_pool)\n",
    "    assert max_pool.shape[1:] == (1,1,channel)\n",
    "    max_pool = shared_layer_one(max_pool)\n",
    "    assert max_pool.shape[1:] == (1,1,channel//ratio)\n",
    "    max_pool = shared_layer_two(max_pool)\n",
    "    assert max_pool.shape[1:] == (1,1,channel)\n",
    "    \n",
    "    cbam_feature = Add()([avg_pool,max_pool])\n",
    "    cbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "    cbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "    \n",
    "    return multiply([input_feature, cbam_feature])\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "    \n",
    "    \"\"\"\n",
    "        Spatial attention module\n",
    "        \n",
    "        Arguments:\n",
    "            \n",
    "            input_feature: input feature map\n",
    "            \n",
    "        Returns:\n",
    "            \n",
    "            product of spatial attention map and input feature map\n",
    "    \"\"\"\n",
    "    \n",
    "    kernel_size = 7\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        channel = input_feature.shape[1]\n",
    "        cbam_feature = Permute((2,3,1))(input_feature)\n",
    "    else:\n",
    "        channel = input_feature.shape[-1]\n",
    "        cbam_feature = input_feature\n",
    "    \n",
    "    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert avg_pool.shape[-1] == 1\n",
    "    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert max_pool.shape[-1] == 1\n",
    "    concat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "    assert concat.shape[-1] == 2\n",
    "    cbam_feature = Conv2D(filters = 1,\n",
    "                    kernel_size=kernel_size,\n",
    "                    strides=1,\n",
    "                    padding='same',\n",
    "                    activation='sigmoid',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    use_bias=False)(concat)\t\n",
    "    assert cbam_feature.shape[-1] == 1\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "        \n",
    "    return multiply([input_feature, cbam_feature])\n",
    "\n",
    "\n",
    "\n",
    "def residual_block(y, nb_channels, _strides=(1, 1), _project_shortcut=False):\n",
    "    \n",
    "  \"\"\"\n",
    "      Residual block (resnet block)\n",
    "        \n",
    "      Arguments: \n",
    "            \n",
    "        y: input feature map\n",
    "        nb_channels: number of channels\n",
    "        _strides: output strides\n",
    "        _project_shortcut: shortcut connection\n",
    "            \n",
    "      Returns:\n",
    "        \n",
    "        output feature map after processing\n",
    "        \n",
    "  \"\"\"\n",
    "\n",
    "  shortcut = y\n",
    "  # down-sampling is performed with a stride of 2\n",
    "  y = Conv2D(nb_channels//4, kernel_size=(3, 3), strides=_strides, padding='same')(y)\n",
    "  y = BatchNormalization()(y)\n",
    "  y = LeakyReLU()(y)\n",
    "\n",
    "  y = Conv2D(nb_channels, kernel_size=(3, 3), strides=(1, 1), padding='same')(y)\n",
    "  y = BatchNormalization()(y)\n",
    "\n",
    "  # identity shortcuts used directly when the input and output are of the same dimensions\n",
    "  if _project_shortcut or _strides != (1, 1):\n",
    "    # when the dimensions increase projection shortcut is used to match dimensions (done by 1×1 convolutions)\n",
    "    # when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2\n",
    "    shortcut = Conv2D(nb_channels, kernel_size=(1, 1), strides=_strides, padding='same')(shortcut)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "  y = add([shortcut, y])\n",
    "  y = LeakyReLU()(y)\n",
    "\n",
    "  return y\n",
    "\n",
    "\n",
    "class BilinearUpsampling(Layer):\n",
    "\n",
    "    \"\"\"\n",
    "        Bilinear Upsampling Class\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, upsampling=(2, 2), data_format=None, **kwargs):\n",
    "\n",
    "        \"\"\"\n",
    "            Constructor of Bilinear-Upsampling\n",
    "        \"\"\"\n",
    "        \n",
    "        super(BilinearUpsampling, self).__init__(**kwargs)\n",
    "        self.data_format = normalize_data_format(data_format)\n",
    "        self.upsampling = conv_utils.normalize_tuple(upsampling, 2, 'size')\n",
    "        self.input_spec = InputSpec(ndim=4)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        height = self.upsampling[0] * \\\n",
    "            input_shape[1] if input_shape[1] is not None else None\n",
    "        width = self.upsampling[1] * \\\n",
    "            input_shape[2] if input_shape[2] is not None else None\n",
    "        return (input_shape[0],\n",
    "                height,\n",
    "                width,\n",
    "                input_shape[3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.image.resize(inputs, (int(inputs.shape[1]*self.upsampling[0]),\n",
    "                                                   int(inputs.shape[2]*self.upsampling[1])))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'size': self.upsampling,\n",
    "                  'data_format': self.data_format}\n",
    "        base_config = super(BilinearUpsampling, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    \n",
    "    \"\"\"\n",
    "        Method to create the final model\n",
    "    \"\"\"\n",
    "    \n",
    "    dropRate = 0.3\n",
    "    \n",
    "    init = Input((224,224,3))\n",
    "\n",
    "    '''\n",
    "    Replaced two 3x3 convolution from 1 7x7 convolution\n",
    "    '''\n",
    "\n",
    "\n",
    "\n",
    "    x = Conv2D(32, (3, 3), activation=None, padding='same')(init) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(32, (3, 3), activation=None, padding='same')(x) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "   \n",
    "\n",
    "    '''\n",
    "    Maxpooling for changed to pool of 3 and stride of 2\n",
    "    inserted padding = same\n",
    "    '''\n",
    "\n",
    "    x1 = MaxPooling2D((3,3),(2,2),padding='same')(x)\n",
    "\n",
    "      \n",
    "    \n",
    "    x = Conv2D(64, (3, 3), activation=None, padding='same')(x1)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = cbam_block(x)\n",
    "\n",
    "    '''\n",
    "    reduced first filter by 4 in residual block\n",
    "    '''\n",
    "\n",
    "    x = residual_block(x, 64)\n",
    "    x2 = MaxPooling2D((3,3),(2,2), padding='same')(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), activation=None, padding='same')(x2)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = cbam_block(x)\n",
    "    x = residual_block(x, 128)\n",
    "    x3 = MaxPooling2D((3,3),(2,2),padding='same')(x)\n",
    "    \n",
    "    x1_aspp = aspp(x1,32,(112,112),1)\n",
    "    x2_aspp = aspp(x2,64,(56,56),1)\n",
    "    x3_aspp = aspp(x3,128,(28,28),1)\n",
    "\n",
    "    ginp1 = UpSampling2D(size=(2, 2), interpolation='bilinear')(x1_aspp)\n",
    "    ginp2 = UpSampling2D(size=(4, 4), interpolation='bilinear')(x2_aspp)\n",
    "    ginp3 = UpSampling2D(size=(8, 8), interpolation='bilinear')(x3_aspp)\n",
    "    \n",
    "    hypercolumn = Concatenate()([ginp1, ginp2, ginp3]) \n",
    "    gap = GlobalAveragePooling2D()(hypercolumn)\n",
    "\n",
    "    x = Dense(256, activation=None)(gap)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(dropRate)(x)\n",
    "    \n",
    "    x = Dense(256, activation=None)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    y = Dense(no_of_classes, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(init, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "for layer in model.layers:\n",
    "    if hasattr(layer, 'kernel_regularizer'):\n",
    "        layer.kernel_regularizer= regularizers.l2(0.03)\n",
    "    \n",
    "    if hasattr(layer, 'bias_regularizer'):\n",
    "        layer.bias_regularizer= regularizers.l2(0.03)\n",
    "\n",
    "\n",
    "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = [\"acc\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFhG4GyaBdks"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "znwQfBexBgNS",
    "outputId": "2058f2ec-be92-48ff-850f-bd8f716b49e3"
   },
   "outputs": [],
   "source": [
    "def scheduler(epoch, lr):\n",
    "   if epoch < 10:\n",
    "     return lr\n",
    "   else:\n",
    "     return lr * tf.math.exp(-0.1)\n",
    "\n",
    "\n",
    "lrate =  tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "history = model.fit(train_data, train_label, batch_size = 4, verbose=1, \n",
    "                         steps_per_epoch = len(train_data)//4, \n",
    "                         epochs = 40,callbacks = [lrate]\n",
    "                        )\n",
    "\n",
    "model.save_weights('/content/drive/MyDrive/LiverNet/KMC_Liver_dataset_folds/Metrics/LiverNet_test_fold5_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWn_d1rGBfvh"
   },
   "source": [
    "### Results and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "soxy0-yOG1Cy",
    "outputId": "85f49351-0f1e-4868-abb0-6aa6d828951c"
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'] , label = 'train_loss')\n",
    "#plt.plot(history.history['val_loss'] , label = 'val_loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"No. of epochs\")\n",
    "plt.ylabel(\"Loss(Categorical Crossentropy)\")\n",
    "plt.title(\"Loss vs. Epoch plot for LiverNet\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['acc'] , label = 'train_acc')\n",
    "#plt.plot(history.history['val_acc'] , label = 'val_acc')\n",
    "plt.legend()\n",
    "plt.xlabel(\"No. of epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs. Epoch plot for LiverNet\")\n",
    "plt.show()\n",
    "\n",
    "print('original')\n",
    "orig_class_indices = np.argmax(test_label,axis=1) \n",
    "print(orig_class_indices)\n",
    "print('\\n')\n",
    "print('predicted')\n",
    "#test_step = test_generator.n//test_generator.batch_size\n",
    "#test_generator.reset()\n",
    "pred = model.predict(test_data ,batch_size=1, verbose = 1)\n",
    "pred_class_indices = np.argmax(pred,axis=1)\n",
    "print(pred_class_indices)\n",
    "print('\\n')\n",
    "\n",
    "#print(train_generator.class_indices)\n",
    "#print('\\n')\n",
    "from sklearn.metrics import precision_score,recall_score,accuracy_score,roc_curve, confusion_matrix, roc_auc_score, auc, f1_score,jaccard_score,classification_report\n",
    "print('Precision {}'.format(precision_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro')))\n",
    "print('Recall {}'.format(recall_score(y_true=orig_class_indices, y_pred=pred_class_indices,average='micro')))\n",
    "print('Accuracy {}'.format(accuracy_score(y_true=orig_class_indices, y_pred=pred_class_indices)))\n",
    "print('F1 {}'.format(f1_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro')))\n",
    "print('jaccard {}'.format(jaccard_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro')))\n",
    "print('confusion_matrix\\n {}'.format(confusion_matrix(y_true=orig_class_indices, y_pred=pred_class_indices)))\n",
    "#print('classification_report\\n {}'.format(classification_report(y_true=orig_class_indices, y_pred=pred_class_indices)))\n",
    "print('\\n\\n')\n",
    "\n",
    "met = np.zeros((no_of_classes+1,5))\n",
    "\n",
    "avg_met = [precision_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro'),\n",
    "           recall_score(y_true=orig_class_indices, y_pred=pred_class_indices,average='micro'),\n",
    "           f1_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro'),\n",
    "           jaccard_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro'),\n",
    "           accuracy_score(y_true=orig_class_indices, y_pred=pred_class_indices)]\n",
    "\n",
    "avg_met = np.array(avg_met)\n",
    "avg_met.round(decimals=2)\n",
    "met[no_of_classes,:] = avg_met\n",
    "\n",
    "classes = classes_name\n",
    "\n",
    "\n",
    "for cl in classes:\n",
    "\n",
    "    print(\"class: \",cl)\n",
    "\n",
    "    a1 = np.uint8(orig_class_indices == cl)\n",
    "    a2 = np.uint8(pred_class_indices == cl)\n",
    "\n",
    "    print('Accuracy {}'.format(accuracy_score(y_true=a1, y_pred=a2)))\n",
    "    print('F1 {}'.format(f1_score(y_true=a1, y_pred=a2)))\n",
    "    print('precision {}'.format(precision_score(y_true=a1, y_pred=a2)))\n",
    "    print('recall {}'.format(recall_score(y_true=a1, y_pred=a2)))\n",
    "\n",
    "    print('jaccard {}'.format(jaccard_score(y_true=a1, y_pred=a2)))\n",
    "    print(\"_______________________________\")\n",
    "\n",
    "    class_met = [precision_score(y_true=a1, y_pred=a2),\n",
    "                 recall_score(y_true=a1, y_pred=a2),\n",
    "                 f1_score(y_true=a1, y_pred=a2),\n",
    "                 jaccard_score(y_true=a1, y_pred=a2),\n",
    "                 accuracy_score(y_true=a1, y_pred=a2)]\n",
    "\n",
    "    class_met = np.array(class_met)\n",
    "    class_met.round(decimals=2)\n",
    "    met[cl,:] =class_met\n",
    "\n",
    "\n",
    "print(met)\n",
    "np.save('/content/drive/MyDrive/LiverNet/KMC_Liver_dataset_folds/Metrics/LiverNet_liver_kmc_met_5.npy',met)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "cm = confusion_matrix(y_true=orig_class_indices, y_pred=pred_class_indices)\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "plot_confusion_matrix(cm, classes_name,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix for IRRCNN',\n",
    "                        cmap=plt.cm.Blues)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "VeuKUG6ZjaVs",
    "outputId": "7eab7f3c-96cf-4501-d0a9-97e31af2e4a2"
   },
   "outputs": [],
   "source": [
    "\n",
    "model.load_weights('/content/drive/MyDrive/LiverNet/KMC_Liver_dataset_folds/Metrics/LiverNet_test_fold5_weights.h5')\n",
    "print('original')\n",
    "orig_class_indices = np.argmax(test_label,axis=1) \n",
    "print(orig_class_indices)\n",
    "print('\\n')\n",
    "print('predicted')\n",
    "#test_step = test_generator.n//test_generator.batch_size\n",
    "#test_generator.reset()\n",
    "pred = model.predict(test_data ,batch_size=1, verbose = 1)\n",
    "pred_class_indices = np.argmax(pred,axis=1)\n",
    "print(pred_class_indices)\n",
    "print('\\n')\n",
    "\n",
    "#print(train_generator.class_indices)\n",
    "#print('\\n')\n",
    "from sklearn.metrics import precision_score,recall_score,accuracy_score,roc_curve, confusion_matrix, roc_auc_score, auc, f1_score,jaccard_score,classification_report\n",
    "print('Precision {}'.format(precision_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro')))\n",
    "print('Recall {}'.format(recall_score(y_true=orig_class_indices, y_pred=pred_class_indices,average='micro')))\n",
    "print('Accuracy {}'.format(accuracy_score(y_true=orig_class_indices, y_pred=pred_class_indices)))\n",
    "print('F1 {}'.format(f1_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro')))\n",
    "print('jaccard {}'.format(jaccard_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro')))\n",
    "print('confusion_matrix\\n {}'.format(confusion_matrix(y_true=orig_class_indices, y_pred=pred_class_indices)))\n",
    "#print('classification_report\\n {}'.format(classification_report(y_true=orig_class_indices, y_pred=pred_class_indices)))\n",
    "print('\\n\\n')\n",
    "\n",
    "met = np.zeros((no_of_classes+1,5))\n",
    "\n",
    "avg_met = [precision_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro'),\n",
    "           recall_score(y_true=orig_class_indices, y_pred=pred_class_indices,average='micro'),\n",
    "           f1_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro'),\n",
    "           jaccard_score(y_true=orig_class_indices, y_pred=pred_class_indices,average = 'micro'),\n",
    "           accuracy_score(y_true=orig_class_indices, y_pred=pred_class_indices)]\n",
    "\n",
    "avg_met = np.array(avg_met)\n",
    "avg_met.round(decimals=2)\n",
    "met[no_of_classes,:] = avg_met\n",
    "\n",
    "classes = classes_name\n",
    "\n",
    "\n",
    "for cl in classes:\n",
    "\n",
    "    print(\"class: \",cl)\n",
    "\n",
    "    a1 = np.uint8(orig_class_indices == cl)\n",
    "    a2 = np.uint8(pred_class_indices == cl)\n",
    "\n",
    "    print('Accuracy {}'.format(accuracy_score(y_true=a1, y_pred=a2)))\n",
    "    print('F1 {}'.format(f1_score(y_true=a1, y_pred=a2)))\n",
    "    print('precision {}'.format(precision_score(y_true=a1, y_pred=a2)))\n",
    "    print('recall {}'.format(recall_score(y_true=a1, y_pred=a2)))\n",
    "\n",
    "    print('jaccard {}'.format(jaccard_score(y_true=a1, y_pred=a2)))\n",
    "    print(\"_______________________________\")\n",
    "\n",
    "    class_met = [precision_score(y_true=a1, y_pred=a2),\n",
    "                 recall_score(y_true=a1, y_pred=a2),\n",
    "                 f1_score(y_true=a1, y_pred=a2),\n",
    "                 jaccard_score(y_true=a1, y_pred=a2),\n",
    "                 accuracy_score(y_true=a1, y_pred=a2)]\n",
    "\n",
    "    class_met = np.array(class_met)\n",
    "    class_met.round(decimals=2)\n",
    "    met[cl,:] =class_met\n",
    "\n",
    "\n",
    "print(met)\n",
    "#np.save('/content/drive/MyDrive/LiverNet/KMC_Liver_dataset_folds/Metrics/LiverNet_liver_kmc_met_5.npy',met)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "cm = confusion_matrix(y_true=orig_class_indices, y_pred=pred_class_indices)\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "plot_confusion_matrix(cm, classes_name,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix for LiverNet',\n",
    "                        cmap=plt.cm.Blues)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "KMC_Liver_cross_val_5(test5).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
